ğŸ§  Remote LLM Inference with Kaggle + Streamlit

This project demonstrates how to run a large language model (LLM) remotely on Kaggle GPU and interact with it using a local Streamlit frontend.

The goal is to separate model inference from the user interface, simulating a real-world AI deployment setup.

ğŸš€ Project Overview
Architecture
Local Machine (Streamlit UI)
|
|  HTTP Requests (ngrok)
v
Kaggle GPU (FastAPI + LLM)


The backend runs on Kaggle using GPU

The frontend runs locally using Streamlit

Communication happens via a REST API

No heavy models are run locally

ğŸ§© Components
ğŸ”¹ Backend (Kaggle)

FastAPI server

Large Language Model: mistralai/Mistral-Nemo-Instruct-2407

GPU inference using PyTorch

API key authentication

Public access via ngrok

ğŸ”¹ Frontend (Local)

Streamlit web interface

Prompt input + token length control

Sends requests to the remote API

Displays generated responses

ğŸ“‚ Project Structure
.
â”œâ”€â”€ backend.py          # FastAPI backend (run on Kaggle)
â”œâ”€â”€ streamlit_app.py    # Streamlit frontend (run locally)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Backend Setup (Kaggle)

Create a Kaggle Notebook with GPU enabled

Install dependencies:

pip install fastapi uvicorn pyngrok transformers accelerate torch


Run backend.py

Copy the generated ngrok public URL, for example:

https://abc123.ngrok-free.app


The backend exposes:

POST /generate

ğŸ’» Frontend Setup (Local)

Install dependencies:

pip install streamlit requests


Update the backend URL in streamlit_app.py:

URL = "https://abc123.ngrok-free.app/generate"


Run Streamlit:

streamlit run streamlit_app.py


Open the browser at:

http://localhost:8501

ğŸ” API Authentication

Requests must include an API key in the header:

Authorization: Bearer secret123


This is handled automatically in the Streamlit frontend.

ğŸ§ª Example API Request
import requests

URL = "https://YOUR_NGROK_LINK/generate"
headers = {"Authorization": "Bearer secret123"}
payload = {
"prompt": "Who is Lionel Messi?",
"max_length": 300
}

res = requests.post(URL, headers=headers, json=payload)
print(res.json()["response"])

ğŸ“Œ Key Learnings

Deploying ML models as REST APIs

Clientâ€“server architecture for AI systems

Using remote GPUs efficiently

Integrating FastAPI with Streamlit

Moving beyond notebook-only ML workflows

âš ï¸ Notes & Limitations

Kaggle notebooks may sleep, causing the API to go offline

ngrok URLs change on every restart

Initial request may be slow due to model loading

ğŸ”® Possible Improvements

Token-by-token streaming

Chat memory support

Better authentication (JWT)

Persistent deployment (HF Spaces / VPS)

UI enhancements (chat-style interface)

ğŸ™ Acknowledgment

Special thanks to the company/team that taught and guided me â€” the practical knowledge and support I received played a key role in building this project.

ğŸ“¬ Feedback

Feedback, suggestions, and improvements are always welcome!
